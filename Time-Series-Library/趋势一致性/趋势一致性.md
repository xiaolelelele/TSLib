1. 训练集、测试集如何划分
2. 0.pdf如何绘制
3. 预测的逻辑是什么：单变量自回归吗？


m4 Hourly 48
真实值数据形状: (414, 48)
预测值数据形状: (414, 48)
19458
趋势一致的比例: 60.91%

m4 daily 14
真实值数据形状: (4227, 14)
预测值数据形状: (4227, 14)
54951
趋势一致的比例: 48.21%

Monthly  18
真实值数据形状: (48000, 18)
预测值数据形状: (48000, 18)
816000
趋势一致的比例: 53.25%

Quarterly
真实值数据形状: (24000, 8)
预测值数据形状: (24000, 8)
168000
趋势一致的比例: 57.55%

Weekly
真实值数据形状: (359, 13)
预测值数据形状: (359, 13)
4308
趋势一致的比例: 53.32%

Yearly
真实值数据形状: (23000, 6)
预测值数据形状: (23000, 6)
115000
趋势一致的比例: 68.30%

# weather 数据集
weather.  96预测96 *2 720  整体趋势一致性  timesNet归一化
趋势一致的比例: 47.05%
mse:0.1679605394601822, mae:0.21928687393665314
timesNet 非归一化  
mse:2748.7900390625, mae:15.319618225097656, dtw:Not calculated
只考虑前三步的趋势一致性： 46.96%


## 单独分类 （错误，只预测了一步）
Dlinear accuracy:0.6067362428842504
transformer  Test Acc: 0.581

先预测  

-1 0 1
0.1 0.2 0.7
0.05 0.9 0.05
0.7 01 02
0.7*-1

lstm训练结果：
趋势分类准确率: 0.4898
mse:0.002123831072822213, mae:0.03313310444355011, dtw:Not calculated,classify_acc:0.4897957758840802

下一步先修正 mse的计算。然后进行比较，timesnet mse：0.172   mae：0.220
+ 考虑magn前的系数只为标签，而不是标签*概率
+ 考虑将lstm换掉

找到问题：mag并未训练。原因在于，构建 delta时，先归一化后相减。因而出现了delta很小的问题
mag_pred tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
解决：改为先相减，再归一化。

pred还原需要反归一化mag，先忽略这部分ytrue和 ypred的loss

换electricity和traffic数据集

## 2025.7.3 先让分类和mag学起来


一个简单的lstm分类  Final Test Loss: 0.8542, Test Accuracy: 54.1992% (包含y)
dlinear  Test Accuracy: 51.5057% (normal)     accuracy:0.4919202013915486(no-normal)
crossformer accuracy:0.5321606105578961


分类+预测  mse:7430.98583984375, mae:75.53792572021484, dtw:Not calculated,classify_acc:0.49474678443763564


## weather long_class  Exp_Long_Term_Forecast_Class
原论文：0.172 0.220  趋势一致：
weather.  96预测96 *2 720  整体趋势一致性 
趋势一致的比例: 47.05%
mse:0.1679605394601822, mae:0.21928687393665314
只考虑前三步的趋势一致性： 46.96%
### 加上分类loss  超参数a=1  logits=[-diff,  0, diff]
47.58%    mse 0.1786   mae:0.228 
### a=2
趋势一致的比例: 45.72%  mse:0.1833302080631256, mae:0.2331942468881607, dtw:Not calculated  

### logits = mlp   超参数a=1
loss: 1.847739815711975
criterion(outputs, batch_y) tensor(0.7539, device='cuda:0', grad_fn=<MseLossBackward>)
self.compute_trend_loss(outputs, batch_y, mode='margin', epsilon=0.1) tensor(1.3710, device='cuda:0', grad_fn=<NllLossBackward>)

a=1
趋势一致的比例: 47.00%
mse:0.1693795770406723, mae:0.22092203795909882, dtw:Not calculated  trend_statistic:0.405789128124793

---------------------------------
## traffic
timesnet
mse:0.5880060195922852, mae:0.3145844340324402   趋势一致的比例: 80.77%
趋势标签统计（真实值）:
  -1: 147511213 (52.78%)
  0: 5091449 (1.82%)
  1: 126887908 (45.40%)
总趋势数: 279490570
### 直接映射
mse:0.5971230864524841, mae:0.33307045698165894, dtw:Not calculated,trend_statistic:0.8326060446332769
### logits = mlp   超参数a=1
timesnet+分类损失
mse:0.5903193354606628, mae:0.31537848711013794, dtw:Not calculated,trend_statistic:0.8082222130070434
激活层改为tanh（有正有负），加上Norm，避免差分diff值过小 超参数a=1
mse:0.5920302271842957, mae:0.3165670931339264, dtw:Not calculated,trend_statistic:0.8079027711024382

mse:0.6085414886474609, mae:0.3432215452194214, dtw:Not calculated,trend_statistic:0.8317701309206962

mse:0.5910674333572388, mae:0.3183048665523529, dtw:Not calculated,trend_statistic:0.7889000154817388
趋势数: 279490570
mse:0.6117299199104309, mae:0.3205370008945465, dtw:Not calculated,trend_statistic:0.7926159369169414
(TSLib) 
mlp的logits学习情况
d_pred_norm tensor([-0.2852, -0.3083, -0.8973, -0.4133, -0.1654, -0.0853, -0.0782, -0.2439,
        -0.5941, -0.1641], device='cuda:0')
logits tensor([[-0.1047, -0.2276,  0.3172],
        [-0.1063, -0.2299,  0.3037],
        [-0.1378, -0.2894, -0.0624],
        [-0.1134, -0.2409,  0.2411],
        [-0.0960, -0.2161,  0.3856],
        [-0.0899, -0.2090,  0.4298],
        [-0.0894, -0.2084,  0.4336],
        [-0.1017, -0.2235,  0.3412],
        [-0.1243, -0.2602,  0.1296],
        [-0.0959, -0.2159,  0.3864]], device='cuda:0')
label_true tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')
d_pred_norm tensor([3.9611, 4.0242, 2.3545, 2.2053, 3.3931, 0.4028, 0.9391, 0.9875, 0.9815,
        1.5288], device='cuda:0')
logits tensor([[-0.4993, -1.1786, -0.3090],
        [-0.5004, -1.1792, -0.3127],
        [-0.4412, -1.1211, -0.1615],
        [-0.4305, -1.1079, -0.1440],
        [-0.4866, -1.1695, -0.2685],
        [-0.1029, -0.7374,  0.0293],
        [-0.2432, -0.8792, -0.0180],
        [-0.2544, -0.8915, -0.0221],
        [-0.2530, -0.8900, -0.0216],
        [-0.3563, -1.0130, -0.0699]], device='cuda:0')
label_true tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')
d_pred_norm tensor([-0.3095,  0.0763,  0.9157, -0.1260,  0.1458,  1.5059,  1.5782,  0.8423,
         2.0634,  0.4106], device='cuda:0')
logits tensor([[ 0.0153,  0.0152,  0.2426],
        [ 0.0021,  0.1138,  0.3526],
        [-0.0838,  0.2710,  0.4816],
        [ 0.0130,  0.0643,  0.2987],
        [-0.0034,  0.1297,  0.3689],
        [-0.1195,  0.3296,  0.5376],
        [-0.1214,  0.3340,  0.5449],
        [-0.0768,  0.2606,  0.4744],
        [-0.1254,  0.3528,  0.5954],
        [-0.0296,  0.1858,  0.4202]], device='cuda:0')
label_true tensor([0, 0, 2, 0, 0, 2, 2, 2, 2, 2], device='cuda:0')


d_pred tensor([-0.1620, -0.2136, -0.3562, -0.2562, -0.1850, -0.2729, -0.2480, -0.2481,
        -0.3301, -0.1989], device='cuda:0')
d_pred_norm tensor([-0.6085, -0.6078, -1.2426, -0.7745, -0.4360, -0.5031, -0.4755, -0.6605,
        -1.0004, -0.5180], device='cuda:0')
logits tensor([[-0.6704, -0.6777,  0.3474],
        [-0.6703, -0.6776,  0.3478],
        [-0.7080, -0.7420, -0.0381],
        [-0.6837, -0.7002,  0.2455],
        [-0.6538, -0.6523,  0.4502],
        [-0.6605, -0.6623,  0.4107],
        [-0.6578, -0.6582,  0.4270],
        [-0.6748, -0.6850,  0.3157],
        [-0.6977, -0.7253,  0.1064],
        [-0.6620, -0.6645,  0.4018]], device='cuda:0')
label_true tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')
d_pred tensor([0.7225, 0.7874, 0.4530, 0.5551, 1.0252, 0.3680, 0.4993, 0.3329, 0.3098,
        0.5243], device='cuda:0')
d_pred_norm tensor([3.6741, 3.5172, 1.5743, 1.9309, 2.8913, 0.6560, 1.0636, 0.9356, 1.0216,
        1.8742], device='cuda:0')
logits tensor([[-0.1112, -0.2794,  1.4577],
        [-0.1259, -0.2817,  1.4439],
        [-0.3599, -0.3969,  1.1831],
        [-0.3081, -0.3600,  1.2491],
        [-0.1894, -0.2978,  1.3814],
        [-0.5037, -0.5053,  0.9473],
        [-0.4393, -0.4566,  1.0663],
        [-0.4596, -0.4719,  1.0318],
        [-0.4459, -0.4617,  1.0553],
        [-0.3161, -0.3654,  1.2393]], device='cuda:0')
label_true tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')